\documentclass[10pt]{article}

%\usepackage[top=1in,left=1.75in,footskip=0.75in]{geometry}
\usepackage[top=1in,left=1in,footskip=0.75in]{geometry}
\usepackage{amsmath,amssymb} % useful for mathematical formulas and symbols
\usepackage{pifont}
\usepackage{blindtext}
\usepackage{tablefootnote}
\usepackage{authblk}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage[utf8x]{inputenc} % Use Unicode characters when possible
\usepackage{textcomp,marvosym} % textcomp package and marvosym package for additional characters
\usepackage{cite} % cite package, to clean up citations in the main text. Do not remove.
\usepackage{nameref} % Use nameref to cite supporting information files
\usepackage[right]{lineno} % line numbers
\usepackage{microtype} % ligatures disabled
\DisableLigatures[f]{encoding = *, family = * }
\usepackage{epstopdf}
\usepackage[table]{xcolor} % color can be used to apply background shading to table cells only
\usepackage{array} % array package and thick rules for tables
\usepackage{bm} % bold math symbols package
\usepackage{graphicx} % nice figures and captions
\usepackage[hidelinks=true]{hyperref} % no green squares around references
\raggedright
\setlength{\parindent}{0.5cm}
%\textwidth 5.25in 
%\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}
% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


\title{Nearest-neighbor Projected-Distance Regression (NPDR) for detecting network interactions with control for multiple tests and confounding} 
\author[1]{Trang T. Le}
\author[2]{Bryan A. Dawkins}
\author[2,3*]{Brett A. McKinney}
\affil[1]{Department of Biostatistics, Epidemiology and Informatics,
University of Pennsylvania, Philadelphia, PA 19104}
\affil[2]{Department of Mathematics, University of Tulsa, Tulsa, OK 74104}
\affil[3]{Tandy School of Computer Science, University of Tulsa, Tulsa, OK 74104}
\renewcommand{\Authands}{ and }
\date{\today}


\begin{document}
\maketitle
\begin{abstract}

Efficient machine learning methods are needed to detect complex interaction-network effects in complicated modeling scenarios in high-dimensional data, such as GWAS, gene expression, eQTL, and structural/functional neuroimage studies for case-control or continuous outcomes.
Many machine learning feature selection methods have limited ability to address the issues of controlling the false discovery rate and adjusting for covariates.
To address these challenges, we develop a new feature selection technique called Nearest-neighbor Projected-Distance Regression (NPDR) that uses the generalized linear model (GLM) to perform regression between nearest-neighbor pair distances projected onto predictor dimensions.
Motivated by the nearest-neighbor mechanism in Relief-based algorithms, NPDR captures the underlying interaction structure of the data, handles both dichotomous and continuous outcomes and various combinations of predictor data types, statistically corrects for covariates and permits regularization.
We use realistic simulations with main effects and network interactions to show that NPDR outperforms standard Relief-based methods and random forest at detecting functional variables while also enabling covariate adjustment and multiple testing correction.
Using RNA-Seq data from a study of major depressive disorder (MDD), we show that NPDR with covariate adjustment effectively removes spurious associations due to confounding.
We apply NPDR to eQTL data to identify potentially interacting variants that regulate transcripts associated with MDD and demonstrate NPDR's utility for GWAS and continuous outcomes.
\\
% \textbf{Availability:} Code and data available at {{https://insilico.github.io/npdr/}}.\\
% \textbf{Contact:} {{brett.mckinney@gmail.com}}\\
% \textbf{Supplementary information:} Supplementary data are available online.

\end{abstract}
% \end{titlingpage}

% \linenumbers

\section*{Significance} Interactions between genes in high dimensional genomic data play an important role in understanding complex biological systems. We propose and validate a flexible machine learning method that detects variables involved in network interactions in numerous types of data while correcting for false discoveries, confounding variables and correlation through regularization. We apply this projected nearest neighbor method to genome-wide association and transcriptomic data, but the new method is broadly applicable to high dimensional data in many fields beyond biology.    

\newpage

\section{Introduction}

Epistasis is a measure of the effect of two genetic variants on a phenotype beyond what would be expected by their independent effects.
There is evidence that these non-independent effects are pervasive~\cite{breen12} and that higher-order interactions also play an important role in genetics~\cite{weinreich13}.
A similar interaction effect can be observed in differential co-expression, where the phenotypic effect of one gene is modified depending on the expression of another gene~\cite{lareau15,diffcoexp10}.
The embedding of these interactions in a regulatory network may lead to, not only pairwise interactions, but also higher-order epistasis network effects.
Explicit modelling of these higher-order interactions would be computationally and statistically intractable~\cite{riesselman18}.
Thus, computationally scalable feature selection methods are needed to capture these higher-order effects in high-dimensional data such as genome-wide association~\cite{titv} and RNA-Seq studies~\cite{stir}.

Relief-based algorithms are efficient nearest-neighbor feature selection methods that are able to detect epistasis or statistical interaction effects in high-dimensional data without resorting to pairwise modeling of attributes~\cite{urbanowicz17b, kononenko97, mckinney09, robnik2003theoretical}.
We recently introduced the STatistical Inference Relief (STIR) formalism~\cite{stir} to address the lack of a statistical distribution for hypothesis testing and the related challenge of controlling the false positive rate of Relief-based scores.
STIR extended Relief-based methods to compute statistical significance of attributes in dichotomous outcome data (e.g., case-control) by reformulating the Relief weight~\cite{mckinney13} as a pseudo t-test for the difference of means between projected-distances onto a given attribute.
STIR is an effective approach with high power and low false-positive rates for data with main and interaction effects and is applicable to any predictor data type (continuous/gene expression or nominal/genetic variants).
However, being based on a t-test, it does not apply to data with a continuous outcome (e.g., quantitative trait) and does not correct for covariates.

In the current study, we introduce a new Nearest-neighbor Projected-Distance Regression (NPDR) approach that extends the STIR formalism to regression of nearest neighbors with the generalized linear model (GLM).
For each attribute, the NPDR model fits a GLM of projected-distances (onto the attribute) between all pairs of nearest-instance neighbors.
% For dichotomous phenotype models (Eq.~\ref{eq:too_logit}), the projected distances between instances are binary match/mismatches.
% For continuous outcome models (Eq.~\ref{eq:lin_reg}), the projected distances are arithmetic differences.
The model of projected distances is linear for continuous outcomes and logistic for dichotomous outcomes, and either model may include corrections for covariates.
The importance of an attribute is given by its standardized regression coefficient and the statistical significance by its P value.
Prior to the current study, no Relief-based method has included covariate correction.

Covariate adjustment is often neglected in machine learning, yet many biological and clinical omic studies involve potentially confounding covariates such as sex, bmi, and age~\cite{le18_brainagesim} or population stratification~\cite{popstrat16}.
Some proposed methods for correcting machine learning algorithms include restricted permutation~\cite{rao2017}, inverse probability weighting of training samples~\cite{linn2016} and penalized support vector machines~\cite{li2011ccsvm}.
Our NPDR framework leads to a natural way to control for covariates by including additional terms for the between-neighbor projected differences for each covariate within the GLM.
We demonstrate the effectiveness of NPDR to correct for confounding in an RNA-Seq study of major depressive disorder (MDD) in which there is a strong signal in the expression data due to the sex of study participants\cite{mostafavi14}.

The flexible GLM formalism of NPDR opens up Relief-based methods to statistical inference for a broad class of problems.
It can detect main effects and interactions for dichotomous and continuous outcome studies while adjusting for covariates and multiple hypothesis testing.
The models allow any predictor data type such as GWAS variants or RNA-Seq expression levels.
NPDR also improves attribute importance estimation compared to other Relief methods because it includes dispersion of the projected distances.
The projected distance model gives the appearance of being univariate but implicitly accounts for interactions with all other attributes via the neighborhood calculation in the space of all attributes (omnigenic).
Further, we demonstrate that the NPDR formalism allows for Lasso and Ridge-like penalized feature selection for Relief-based methods.

The paper is organized as follows.
In the Methods section, we develop the new formalism of NPDR to reformulate Relief-based scores as coefficients in a distanced-based GLM.
% For data with continuous outcome, we use linear regression, which has theoretical similarities to RRelief but better statistical properties and enables covariate correction.
% For dichotomous outcome data, we employ a logistic model in the GLM along with possible covariate terms.
We use the projected-distance regression formalism to implement a penalized version of NPDR.
In the Results, we use realistic simulations with main effects and network interactions to demonstrate improved feature selection performance over standard Relief-based methods and random forest.
We apply NPDR to an RNA-Seq and GWAS study of MDD.
% : one with MDD diagnostic status and the other with a continuous outcome (Pittsburgh Sleep Quality Index, PSQI).
From the RNA-Seq data, we show that NPDR removes spurious associations with MDD due to confounding by sex and identifies biologically relevant genes.
Combining the RNA-Seq with the GWAS data, we perform an eQTL analysis with NPDR to identify potentially interacting variants that regulate transcripts associated with MDD and demonstrate NPDR's utility for GWAS and continuous outcomes.

%Residualize? r=glm(trait~1, offset=c*t, family=binomial(link=probit))\$residuals;fit=lm(r~genotypes) 
%plink --bfile data --linear --covar covars.txt --adjust --out data


% \begin{methods}
\section{Materials and Methods}
In this section, we develop the mathematical formalism needed to describe the projected distance regression in NPDR.
We then construct the NPDR GLM models for common analysis situations, including continuous and dichotomous outcomes and adjustment for covariates.
We also describe the simulation approach and real datasets for method validation.

\def\ri{R_i}
\def\rj{R_j}
\def\kmi{k_{M_i}}
\def\khi{k_{H_i}}
\def\hji{H_{j_i}}
\def\ma{\overline{M}_a}
\def\ha{\overline{H}_a}
\def\mnu{M_\nu}
\def\hnu{H_\nu}
\def\myd{\text{diff}}
\def\ka{\bar{k}_\alpha}

\subsection{Distance metrics and nearest neighbors}\label{sec:reform}
Because NPDR and other Relief-based feature selection methods are based on distances between instances, we first describe the algorithms and notation for identifying nearest neighbors in the space all attributes.
We use the term attribute to refer to predictor variables, which may be continuous (e.g., expression) or categorical (e.g., variants).
We use the term instance to refer to samples or subjects in a dataset.

\subsubsection{Distances and projections onto attributes}
The distance between instances $i$ and $j$ in the data set $X^{m \times p}$ of $m$ instances and $p$ attributes is calculated in the space of all attributes ($a \in A$, $|A|=p$) using a metric such as
\begin{equation}\label{eq:D}
% D^{(q)}_{ij}=\left(\sum_{a\in A}|\text{d}^{\text{type}}_{ij}(a)|^q\right)^{1/q},
D^{(q)}_{ij}=\left(\sum_{a\in A}|\text{d}_{ij}(a)|^q\right)^{1/q},
\end{equation}
which is typically Manhattan ($q=1$) but may also be Euclidean ($q=2$).
The quantity 
% $\text{d}^{\text{type}}_{ij}(a)$, 
$\text{d}_{ij}(a)$,
known as a ``$\text{diff}$'' in Relief literature, is the projection of the distance between instances $i$ and $j$ onto the attribute $a$ dimension.
The function $\text{d}_{ij}(a)$ supports any type of attribute (e.g., numeric/continuous versus categorical).
% ``type'' refers to the data type of the attribute
For example, the projected difference between two instances $i$ and $j$ for a continuous numeric ($\text{d}^{\text{num}}$) attribute, $a$, may be defined as
%\begin{equation}\label{eq:diff}
%\text{diff}^{(\text{num})}(a,(\ri,\rj))=\frac{|\text{value}(a,\ri)-\text{value}(a,\rj)%|}{\max(a)-\min(a)}.
%\end{equation}
\begin{equation}\label{eq:diff}
\begin{aligned}
\text{d}^{\text{num}}_{ij}(a)&=\text{diff}(a,(i,j))\\
                                            & = {|\hat{X}_{ia}-\hat{X}_{ja}|},
\end{aligned}
\end{equation}
where $\hat{X}$ represents the standardized data matrix $X$.
We use a simplified d$_{ij}(a)$ notation in place of the $\text{diff}(a,(i,j))$ notation that is customary in Relief-based methods.
We also omit the division by $\max(a)-\min(a)$ that is used by Relief to constrain scores to the interval from $-1$ to $1$.
As we show in subsequent sections, NPDR attribute importance scores are standardized regression coefficients with corresponding P values, so any scaling of the projected distances is unnecessary for comparing attribute scores.
%\emph{Omit: The scaling may alleviate bias in the distance calculation. However, standardizing the data matrix $X$ ($\hat{X}$) should have the same effect without division by $\max(a)-\min(a)$, which has usual distribution properties for distances (expand).}
Thus, for the numeric-data projection, d$^{\text{num}}_{ij}(a)$, we simply use the absolute difference between row elements $i$ and $j$ of the data matrix $X^{m \times p}$ for the attribute column $a$.

This numeric projection function (Eq.~\ref{eq:diff}) is appropriate for gene expression and other quantitative predictors and outcomes.
For genome-wide association study (GWAS) data, where attributes are categorical, one simply modifies the type in the projection function~\cite{titv}, but the projected-distance regression methods will be otherwise unchanged.
The $\text{d}_{ij}(a)$ quantity is typically part of the metric to define the neighborhood, but it is also essential for computing the importance coefficients (Sec. \ref{sec:regress}).
The projected-distance regression models below (Eqs.~\ref{eq:lin_reg},~\ref{eq:lin_reg_cov},~\ref{eq:logit_nocovar}, and~\ref{eq:too_logit}) will be fit for all nearest neighbors $i$ and $j$ in the defined neighborhood (discussed next in Eq.~(\ref{eq:N})).

\subsubsection{Nearest-neighbor ordered pairs}
In the original Relief-F approach for dichotomous outcome data, two neighborhood sets are calculated: one for hits (instances in the same class) and one for misses (instances in different classes).
For NPDR, only one neighborhood is needed, regardless of whether the problem is classification or regression and regardless of whether a fixed-$k$ or adaptive radius neighborhood method is used~\cite{greene09,urbanowicz17,mckinney13}.
The NPDR neighbors are chosen blind to the outcome variable and then pairs of instances are assigned to hit or miss groups for dichotomous outcome data and assigned numeric differences for quantitative outcome data.
This blinded selection leads to less overfitting of the neighborhood boundaries and less bias for imbalanced data.

We define the NPDR neighborhood set $\mathcal{N}$ of ordered-pair indices as follows.
Instance $i$ is a point in $p$ dimensions, and we designate the topological neighborhood of $i$ as $N_{i}$.
This neighborhood is a set of other instances trained on the data $X^{m \times p}$ and depends on the type of Relief neighborhood method (e.g., fixed-$k$ or adaptive radius) and the type of metric (e.g., Manhattan or Euclidean).
If instance $j$ is in the neighborhood of $i$ ($j \in N_{i}$), then the ordered pair is in the overall neighborhood ($(i,j) \in \mathcal{N}$) for the projected-distance regression analysis.
The ordered pairs constituting the overall neighborhood can then be represented as nested sets:
\begin{equation}\label{eq:N}
\mathcal{N}=\{\{(i, j)\}_{i=1}^{m}\}_{\{j \ne i : j \in N_{i}\}}.
\end{equation}
The cardinality of the set $\{j \ne i : j \in N_{i}\}$ is $k_i$, the number of nearest neighbors for subject $i$.

%Note that the inner index $j$ depends on the outer index $i$. This is important for multiSURF, where each instance $\ri$ will, in general, have a different number of misses and hits ($\kmi$ and $\khi$) and these values may differ between instances.  Thus, for multiSURF, the sets $M$ and $H$ can be thought of as irregular or ragged matrices of ordered pairs. For ReliefF algorithms, where the number of neighbors is constant across subjects, the hit and miss matrices are proper (non-ragged) matrices of ordered pairs. % We further discuss how we utilize the reshaped ordered pairs in the next subsection.

\subsubsection{Adaptive-radius and fixed-k Neighborhoods}
The NPDR algorithm applies to any Relief neighborhood algorithm.
In the applications in the current study, we use the multiSURF~\cite{urbanowicz17} adaptive radius neighborhood, which uses a different radius for each instance, and we use a fixed-$k$ neighborhood that well-approximates mulitSURF, which is derived in Ref.~\cite{bod}.
The adaptive radius for an instance may be defined as the mean of its distances to all other instances subtracted by the fraction $\alpha$ of the standard deviation of this mean.
More precisely, an instance $j$ is in the adaptive $\alpha$-radius neighborhood of $i$ ($j \in N^{\alpha}_{i}$) under the condition
%\begin{equation}
%\bar{D = \frac{2p}{\sqrt{\pi}}
%\end{equation}
%
%\begin{equation}
%\sigma_{D} = \frac{2p(\pi-2)}{\pi}
%\end{equation}
%
\begin{equation}
D_{ij} \le \, R_i^{\alpha} \implies j \in N^{\alpha}_{i},
\end{equation}
where the threshold radius for instance $i$ is
\begin{equation} 
R_i^{\alpha} =  \bar{D}_i - \alpha \, \sigma_{\bar{D}_i}
\end{equation}
and 
\begin{equation}
\bar{D}_i = \frac{1}{m-1} \sum_{j \ne i} D^{(\cdot)}_{ij}
\end{equation}
is the average of instance $i$'s pairwise distances (using Eq. \ref{eq:D}) with standard deviation $\sigma_{\bar{D}_i}$.
MultiSURF uses $\alpha=1/2$~\cite{msurf13}.
 
Previously we showed empirically for balanced dichotomous outcome datasets that a good constant-$k$ approximation to the expected number of neighbors within the multiSURF radii is $k=m/6$~\cite{stir}, where $m$ is the number of samples.
In Ref.~\cite{bod} we derive a more exact theoretical mean that shows the mathematical connection between fixed-$\alpha$ and fixed-$k$ neighbor-finding methods, which is given by 
\begin{equation}\label{eq:kbar}
{\bar{k}}_{\alpha} = \left \lfloor \frac{m-1}{2}  \left( 1 - \mathrm{erf}\left( \frac{\alpha}{\sqrt{2}} \right) \right) \right \rfloor,
\end{equation}
where we apply the floor to ensure the number of neighbors is integer.
For data with balanced hits and misses in standard fixed-$k$ Relief, one further divides this formula by 2, and then for multiSURF ($\alpha=1/2$), we find $\bar{k}_{1/2}^{\text{hit/miss}} = \frac{1}{2}\bar{k}_{1/2} = 0.154 (m-1)$, which is very close to our previous empirical estimate $m/6$.
In the current study, when we compare multiSURF neighborhood methods with fixed-$k$ neighborhoods, we use $\bar{k}_{1/2}$.
Using this $\alpha=1/2$ value has been shown to give good feature selection performance by balancing power for main effects and interaction effects.
However, the best value for $\alpha$ or $k$ is likely data-specific and may be determined through nested cross-validation and other parameter tuning methods~\cite{bod}.

\subsection{Nearest-neighbor Projected-Distance Regression (NPDR) with the generalized linear model}

\subsubsection{Continuous outcomes: linear regression NPDR}\label{sec:regress}

Once the neighborhood $\mathcal{N}$ (Eq.~\ref{eq:N}) is determined by the distance matrix $D_{ij}$ (Eq. \ref{eq:D}) and the neighborhood method is chosen (e.g., fixed number of neighbors $k$ or adaptive radius), we can compute the NPDR test statistic and P value for the association of an attribute with the phenotype.
The NPDR model predictor vector is the attribute's projected distances ($\text{d}_{ij}(a)$) between all pairs of nearest-neighbor instances ($(i,j) \in \mathcal{N}$).
For continuous outcome data (quantitative phenotypes), the NPDR model outcome vector is the numeric difference (Eq. \ref{eq:diff}) between all nearest neighbors $i$ and $j$.
We find the parameters of the following model that minimize the least-squares error over $\forall(i,j) \in \mathcal{N}$: 

%We showed in Ref. (\cite{mckinney13}) that the ReliefF importance weight for an attribute, $a$, can be expressed as a difference of mean %diffs between hit and miss groups. Here we extend this difference to any Relief-based neighborhood scheme.  

%\begin{equation}
%    y_{j} = \beta_{o} + \beta_{a} X_{ja} + \epsilon_{j}
%\end{equation}
%
%\begin{equation}
%   y_{j} = \beta_{o} + \beta_{a} a_j + \epsilon_{j}
%\end{equation}
%
%\begin{equation}
%    j = 1,\ldots,m
%\end{equation}
\begin{equation}\label{eq:lin_reg}
    \text{d}^{\text{num}}_{ij}(y) = \beta_{o} + \beta_{a} \text{d}_{ij}(a) + \epsilon_{ij}.
\end{equation}
%for
%\begin{equation}
%   \forall(i,j) \in \mathcal{N}.
%\end{equation}
The $\text{d}^{\text{num}}_{ij}(y)$ term on the left is the projected distance (diff) between instances $i$ and $j$ for a numeric phenotype $y$ (Eq.~\ref{eq:diff}) and $\epsilon_{ij}$ is the error term for this random variable.
The predictor attribute $a$ may be numeric or categorical, which determines the ``type'' used in the diff function on the right hand side of Eq.(\ref{eq:lin_reg}).
The NPDR test statistic for attribute $a$ is the $\beta_a$ estimate with a one-sided hypothesis
\begin{equation}\label{eq:linreg_null}
\begin{aligned}
    & H_0: \beta_a < 0 \\
    & H_1: \beta_a \ge 0.
\end{aligned}
\end{equation}
The $\beta_a$ can be interpreted as the predicted change in the difference of the quantitative outcome between a pair of subjects when the projected distance of the attribute, $a$, changes by one unit.
The attribute weights in the original RRelief algorithm~\cite{robnik2003theoretical} can be described as a weighted covariance between the attribute neighbor projected distances, $\text{d}_{ij}(a)$, and the outcome neighbor differences, $\text{d}^{\text{num}}_{ij}(y)$.
The extra weighting in RRelief is an exponentially decaying function of the rank of the distance between neighbors.
The NPDR attribute weight is the standardized regression coefficient, $\beta'_a$, which is the covariance of the projected distances divided by the variance of the outcome projected distances.
When the regression contains no additional covariates, the NPDR attribute weight can be written as the correlation between outcome and attribute neighbor projected distances:
\begin{equation}
% \beta_a = \frac{\text{Cov}\left( \bf{\text{d}}(\bf{y}), \bf{\text{d}}(\bf{a}) \right)} {\text{Var}\left( \bf{\text{d}}(\bf{a}) \right)}.
\beta'_a = \text{corr}\left( \bf{\text{d}}(\bf{y}), \bf{\text{d}}(\bf{a}) \right).
\end{equation}
Thus, in the case of no covariates, NPDR for regression and RRelief have similar structure, but, as we show shortly, NPDR provides an improved attribute estimation, and the flexible NPDR framework can include additional sources of variation (i.e., adjust for confounding covariates).

%Could also write in terms of min of -LL. 

\subsubsection{Linear regression NPDR with covariates}
Previous Relief-based methods do not include the ability to adjust for covariates.
The regression formalism of NPDR makes adding covariates straightforward.
We simply compute the projected difference values $\text{d}_{ij}(\vec{y}_{\text{covs}})$ for the covariate attribute(s) between subjects on the neighborhood $(\forall(i,j) \in \mathcal{N})$ and include this as an additional projected distance term in the regression model:

\begin{equation}\label{eq:lin_reg_cov}
    \text{d}^{\text{num}}_{ij}(y) = \beta_{0} + \beta_{a} \text{d}_{ij}(a) + \vec{\beta}^{T}_{\text{covs}}\text{d}_{ij}(\vec{y}_{\text{covs}}) + \epsilon_{ij}.
\end{equation}
%where
%\begin{equation}
%    \forall(i,j) \in \mathcal{N}.
%\end{equation}
The above vector notation for the regression coefficients of the $p_c$ covariates can be expanded as  
\begin{equation}
\vec{\beta}^{T}_{\text{covs}} = \left( \beta_{\text{cov}_1}, \beta_{\text{cov}_2}, \ldots,  \beta_{\text{cov}_{p_c}} \right)
\end{equation}
and the projection differences between instances $i$ and $j$ for each of the $p_c$ covariates can be expanded as
\begin{equation}
\text{d}_{ij}(\vec{y}_\text{covs})= \left( \text{d}^{\text{type}_1}_{ij}({y}_{\text{cov}_1}), \text{d}^{\text{type}_2}_{ij}({y}_{\text{cov}_2}), \ldots, \text{d}^{\text{type}_{p_c}}_{ij}({y}_{\text{cov}_{p_c}}) \right)^{T}.
\end{equation}
The superscripts in the projection operators above indicate the appropriate operator type for each covariate data type (e.g., numeric or categorical).
In addition, the predictor attribute $a$ may be numeric or categorical, which determines the type used in $\text{d}_{ij}(a)$.
The NPDR test statistic is again $\beta_a$ with alternative hypothesis $\beta_a \ge 0$ as in Eq. (\ref{eq:linreg_null}), and the standardized $\beta_a'$ is the attribute importance score. 


\subsubsection{Dichotomous outcomes with covariates: logistic regression NPDR}
The STIR method was designed for statistical testing in dichotomous data, but as we have seen NPDR can also handle continuous outcomes and adjust for covariates.
Here we show that the GLM formalism also enables NPDR to handle dichotomous outcome data (e.g., case-control phenotype).
For dichotomous outcomes, NPDR models the probability $p^{\text{miss}}_{ij}$ that subjects $i$ and $j$ are in the opposite class (misses) versus the same class (hits) from the neighbor projected distances with a logit function.
We estimate the parameters of the following model for neighbors $\forall(i,j) \in \mathcal{N}$:   
%\begin{equation}
%\text{logit}(p^{\text{miss}}_{ij}) = \beta_0 + \beta_a \text{d}^{\text{(num)}}_{ij}(a) + \beta_{\text{sex}} \text{d}^{\text{(miss)}}_{ij}({y}_{\text{sex}}) + \epsilon_{ij},   
%\end{equation}
\begin{equation}\label{eq:logit_nocovar}
\text{logit}(p^{\text{miss}}_{ij}) = \beta_0 + \beta_a \text{d}_{ij}(a) + \epsilon_{ij},   
\end{equation}
or if there are covariates,
\begin{equation}\label{eq:too_logit}
\text{logit}(p^{\text{miss}}_{ij}) = \beta_0 + \beta_a \text{d}_{ij}(a) + \vec{\beta}^{T}_{\text{covs}} \text{d}_{ij}(\vec{y}_{\text{covs}}) + \epsilon_{ij},   
\end{equation}
where $p^{\text{miss}}_{ij}$ is the probability that subjects $i$ and $j$ have different phenotypes given the difference in their values for the attribute, $a$, and given the covariate differences.
The outcome variable that is modeled by probability $p^{\text{miss}}_{ij}$ is a binary difference between subjects for the phenotype ($\vec{y}$):
   
\begin{equation}\label{eq:hitdiff}
\text{d}^{\text{miss}}_{ij}(\vec{y}) = \left\{
    \begin{array}{ll}
        0, & \quad  y_{i} == y_{j} \\
        1, & \quad \text{else}.
    \end{array}
\right.
\end{equation}
The $\beta_a$ statistic can be interpreted in the following way.
For a unit increase in the difference in the value of the attribute between two neighbors, we predict a change of $e^{\beta_a}$ in the odds of the neighbors being in opposite classes.
For dichotomous outcome data, we are interested in the alternative hypothesis that $\beta_a>0$ because negative $\beta_a$ values represent attributes that are irrelevant to classification.
Thus, like NPDR linear regression, we are interested in testing one-sided hypotheses
\begin{equation}
\begin{aligned}
    & H_0: \beta_a \le 0 \\
    & H_1: \beta_a > 0.
\end{aligned}
\end{equation}
Nominal outcomes can be analyzed (similar to multi-state Relief-F) with NPDR by grouping all misses of an instance as one group.
This may be improved by using multinomial regression in NPDR.

\subsubsection{Regularized NPDR}

To complement the multiple-testing adjusted NPDR approach, we develop a regularized NPDR method that combines all of the attribute difference vectors into one design matrix and constrains the coefficients to be non-negative, similar to the one-tailed test we use in standard NPDR.
Specifically, we minimize the vector of regression coefficients, $\vec{\beta}_A = (\beta_{a_1}, \beta_{a_2}, \ldots, \beta_{a_p})$, simultaneously for all attribute projections $a \in A$, subject to the coefficients being non-negative: 

% \begin{equation}\label{eq:glmnetNPDR}
%     \min_{\beta_o, \vec{\beta}_A} \frac{1}{|\mathcal{N}|}  \sum_{i,j \in \mathcal{N}} 
%            \mathcal{L} \left(\text{d}^{\text{miss}}_{ij}(y), \beta_0 + \vec{\beta}_A^{T} \text{d}_{ij}(A) \right)
%      + \lambda \vec{\beta}_A.
% \end{equation}
\begin{equation}\label{eq:glmnetNPDR}
\begin{aligned}
    \min_{\beta_o, \vec{\beta}_A} \frac{1}{|\mathcal{N}|} &  \sum_{i,j \in \mathcal{N}} 
           \mathcal{L} \left(\text{d}^{\text{miss}}_{ij}(y), \beta_0 + \vec{\beta}_A^{T} \text{d}_{ij}(A) \right)
     + \lambda || \vec{\beta}_A ||_1 \\
     & \beta_{a_k} \ge 0, \,\,\,\, k=1,\ldots,p.
     \end{aligned}
\end{equation}
$\mathcal{L}$ is the negative log-likelihood for each pair of instances $i$ and $j$ in neighborhood $\mathcal{N}$, and $\text{d}_{ij}(A)$ represents the vector of diffs for fixed $i$ and $j$ for all attributes $a \in A$: 
\begin{equation}
\text{d}_{ij}(A)= \left( \text{d}^{\text{type}_1}_{ij}(a_1), \text{d}^{\text{type}_2}_{ij}(a_2), \ldots, \text{d}^{\text{type}_p}_{ij}(a_p) \right)^{T}.
\end{equation}
Our implementation uses a zero lower limit for the coefficients and the penalty strength $\lambda>0$ is chosen by cross-validation~\cite{glmnet05}.
For dichotomous outcomes, we use the binomial link function for the hit/miss projected distances in the likelihood optimization.

% We now propose a regularized NPDR approach that combines all of the attribute difference vectors into one design matrix and constrains the optimization of the coefficients with an elasticnet penalty~\cite{glmnet05} for feature selection.
% Specifically, we minimize the vector of regression coefficients, $ \vec{\beta}_A$, for all attributes $a \in A$, that is $\vec{\beta}^{T}_{\text{A}} = (\beta_{a_1}, \beta_{a_2}, \ldots, \beta_{a_p})$, subject to the elastic net penalty: 
% 
% \begin{equation}\label{eq:glmnetNPDR}
% \begin{aligned}
%     \min_{\beta_o, \vec{\beta}_A} \frac{1}{|\mathcal{N}|}  \sum_{i,j \in \mathcal{N}} & 
%            \mathcal{L} \left(\text{d}^{\text{miss}}_{ij}(y), \beta_0 + \vec{\beta}_A^{T} \text{d}_{ij}(A) \right)
%      \\ 
%      & + \lambda \left[ \alpha || \vec{\beta}_A ||_1 + || \vec{\beta}_A ||_2^2 (1-\alpha)/2  \right].
% \end{aligned}
% \end{equation}
% $\mathcal{L}$ is the negative log-likelihood for each pair of instances $i$ and $j$ in neighborhood $\mathcal{N}$, and $\text{d}_{ij}(A)$ represents the vector of diffs for fixed $i$ and $j$ for all attributes $a \in A$: 
% \begin{equation}
% \text{d}_{ij}(A)= \left( \text{d}^{\text{type}_1}_{ij}(a_1), \text{d}^{\text{type}_2}_{ij}(a_2), \ldots, \text{d}^{\text{type}_p}_{ij}(a_p) \right)^{T}.
% \end{equation}
% The elastic-net parameter $\alpha$ mixes the amount of lasso ($\alpha$=1) and ridge ($\alpha$=0) penalty. Our implementation allows any value of $\alpha$, but we use lasso as the default to choose one predictor (attribute diff vector) when a set of predictors is correlated. The overall penalty strength $\lambda$ is chosen by cross-validation. For dichotomous outcome, we use binomial link function for the hit/miss projected distances in the likelihood optimization, which leads to elastic net treating positive and negative $\beta_{a_i}$ coefficients similarly when shrinking. One can simply remove any negative coefficients after shrinkage, but a better approach may be to modify the likelihood to ordinal (one-sided likelihood).   

\subsection{Properties of NPDR and existing Relief-based methods}
Here we summarize the properties and capabilities of standard Relief-based methods\cite{urbanowicz17b} and the generalizations STIR\cite{stir} and NPDR (Table \ref{tab:compare_npdr}).
When there are no covariates for dichotomous outcome data, STIR (based on a pseudo t-test) and NPDR (based on regression with a logit model) are approximately equivalent given reasonable distribution assumptions (see Supplementary Fig. S1).
However, by design, the NPDR framework is more flexible and able to handle covariate adjustment and continuous outcomes.
In the current notation, the STIR null and alternative hypotheses would be

%\begin{equation}
%\begin{aligned}
%    & H^{\text{(stir)}}_0: \bar{M}_a - \bar{H}_a \le 0 \\
%    & H^{\text{(stir)}}_1: \bar{M}_a - \bar{H}_a > 0,
%\end{aligned}
%\end{equation}  

\begin{equation}
\begin{aligned}
    & H^{\text{stir}}_0: \mu_M(a) - \mu_H(a) \le 0 \\
    & H^{\text{stir}}_1: \mu_M(a) - \mu_H(a) > 0,
\end{aligned}
\end{equation}  
where
\begin{equation}
\begin{aligned}
    \mu_M(a) & = \bar{M}_a = E \left( \text{d}_{ij}(a) \cdot \left( 1-\text{d}^{\text{miss}}_{ij}(y) \right) \right) \\
    \mu_H(a) & = \bar{H}_a = E \left( \text{d}_{ij}(a) \cdot        \text{d}^{\text{miss}}_{ij}(y)           \right)
\end{aligned}
\end{equation}  
and $\text{d}^{\text{miss}}_{ij}(y)$ is given by Eq.(\ref{eq:hitdiff}). The STIR test statistic is a pseudo t-test (see Ref~\cite{stir}).

For dichotomous outcomes, STIR improves attribute estimates over Relief weights ($\bar{M}_a - \bar{H}_a$) by incorporating sample variance of the nearest neighbor distances in the denominator, which also enables STIR to estimate statistical significance with the assumptions of a t-test (Table \ref{tab:compare_npdr}).
NPDR assumes intra- and inter-class differences are randomly sampled from one distribution and computes the importance score from a logistic regression $\beta'_a$.
This regression-based generalization improves NPDR's attribute estimates over Relief weights and enables statistical significance estimation for a wider range of problems than STIR.

\begin{table}[h]

\begin{tabular}{p{4.5cm}C{2cm}C{3.2cm}C{2.7cm}}
                           & Standard Relief-based & STIR    & NPDR \\
\hline
Importance score \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, (dichotomous)  & $\bar{M}_a - \bar{H}_a$  & $\dfrac{\bar{M}_a - \bar{H}_a }{S_p(M,H)\sqrt{\frac{1}{|M|}+\frac{1}{|H|}}}$ &  $\beta'_a$ coefficient \\
Score has a null distribution & No                               & Yes     & {\bf Yes}  \\
Supports continuous outcome   & Yes                              & No      & {\bf Yes}  \\
Supports covariates           & No                               & No      & {\bf Yes} \\
\hline
\end{tabular}
\caption{Properties of standard Relief-based methods and generalizations STIR and NPDR. The NPDR score is the standardized regression coefficient $\beta'_a$ from a logistic model (Eq.~\ref{eq:logit_nocovar}) for dichotomous outcomes and from a linear model (Eq.~\ref{eq:lin_reg}) for continuous outcomes. The quantity $S_p$ in STIR is the pooled standard deviation of the hit and miss means. Only the score for dichotomous (hit/miss) Relief is shown and STIR is limited to dichotomous outcomes~\cite{stir}.}
\label{tab:compare_npdr}
\end{table}

% \begin{table}[h]
% 
% \begin{tabular}{p{4.5cm}C{2cm}C{3.2cm}C{2.7cm}}
%                            & Traditional methods & STIR    & NPDR \\
% \hline
% Importance score $W$\tablefootnote{Considering a binary outcome problem.}        & $\bar{M}_a - \bar{H}_a    $                      & $\dfrac{\bar{M}_a - \bar{H}_a }{S_p[M,H]\sqrt{\frac{1}{|M|}+\frac{1}{|H|}}}$ &   $\text{corr}\left(\text{p}_y ,\text{d}_a  \right)$\tablefootnote{where $\text{d}_a = [M_a, H_a]$ and $\text{logit}(\text{p}_y) =  [M_y, H_y]$.} \\
% $W$ has a distribution       & \ding{55}                               & \ding{51}     & \ding{51}  \\
% Supports continuous outcome & \ding{51}                              & \ding{55}      & \ding{51}  \\
% Supports covariates         & \ding{55}                               & \ding{55}      & \ding{51} \\
% \hline
% \end{tabular}
% \caption{Comparison of NPDR, STIR and traditional Relief-based methods.}
% \label{tab:compare_npdr}
% \end{table}

\subsection{Real and simulated datasets}
\subsubsection{Simulation methods}
To compare power and false positive performance for NPDR and other feature selection methods, we use the simulation tool from our private Evaporative Cooling (privateEC) software~\cite{le17} that was designed to simulate realistic main effects, correlations, and interactions found in gene expression or resting-state fMRI correlation data.
For continuous outcome data, we simulate main effects with $m=200$ subjects and $p=1000$ real-valued attributes with 10\% functional (true positive association with outcome).
We choose a sample size consistent with real gene expression data but on the smaller end to demonstrate a more challenging scenario.
Likewise, the effect size parameter ($b=0.8$) was selected to be sufficiently challenging with power approximately 40\%~\cite{le17}.

For dichotomous outcome data ($100$ cases and $100$ controls), we simulate network interactions using the differential co-expression network-based simulation tool in privateEC, which is described in Refs.~\cite{le17, lareau15}.
We first create a co-expression network on an Erd\H{o}s-R\'enyi random graph with $0.1$ attachment probability, which is the critical value for a giant component.
We give connected genes a higher average correlation, approximately $r_{\text{connected}}=0.8$.%($s_{\text{int}}=.8$) ($s_{\text{int}}=.4$ gives $r_{\text{connected}}=0.9$).
This correlation is related to the interaction effect size because we disrupt the correlation of target genes in cases but maintain correlation within controls, thereby creating a final differential correlation network.
%and we create a correlated data matrix by applying a Cholesky transformation of the designed covariance matrix to random normal data of size $m \times p$. 
%Finally, we simulate functional effects on the outcome of functional variables by permuting their values in cases but leaving the controls unpermuted, thereby leading to a final differential correlation network.

All resulting p-values (from STIR and NPDR) are adjusted for multiple testing.
Attributes with adjusted p-values less than 0.05 are counted as a positive test (null hypothesis rejected), else the test is negative.
%[Variables that are not shrunk to zero by regularization (glmnet and regularized NPDR) are counted as positive tests.]
We assess the feature-selection performance of each method by averaging the area under the precision-recall curve (auPRC) across 100 replicates of each simulation scenario.
Assuming relatively few functional attributes ($10\%$ of 1,000 attributes) compared to non-functional ones, the precision and recall measures are robust to imbalanced data and are thus a useful assessment of a method's propensity to assign higher scores to the correct functional attributes.
The auPRC is also a good comparison tool for methods that, unlike NPDR, do not have a statistical significance threshold.
We remark that, even though the auPRC terminology used here is similar to traditional classification problems, we instead focus on evaluating the attribute score quality, not classification accuracy.

\subsubsection{RNA-Seq data and NPDR adjustment for confounding factors}
To test the ability of NPDR to correct for confounding, we apply NPDR to the RNA-Seq study in Ref.~\cite{mostafavi14} that consists of 15,231 genes for 463 MDD cases and 452 controls.
Of the 915 subjects, 641 are female and 274 are male.
The chi-square between MDD and sex is $25.746$ ($p=3.89e-7$), and there are 485 genes significantly associated with sex.
Thus, there is high risk for confounding effects due to sex differences.
We apply NPDR with a multiSURF neighborhood and compute importance scores of all genes with and without sex as a covariate to isolate confounding genes.
%We applied univariate logistic regression with and without sex as a covariate to these genes, and we directly tested each gene for association with sex.
% We also apply NPDR to another RNA-Seq expression dataset with a continuous outcome: Pittsburgh Sleep Quality Index (PSQI). Sleep quality was measured for 78 individuals diagnosed with major depressive disorder (MDD) and 79 healthy controls (HC)\cite{le18}.
% Gene expression levels were quantified from reads of 19,968 annotated protein-coding genes and preprocessed with low read-count removal, technical adjustment, and coefficient of variation filtering.
% As a result, 5,912 RNA-Seq measures were obtained and used as input in the NPDR model to predict sleep quality by PSQI\cite{psqi}.
% All P values are adjusted for multiple testing.

\subsubsection{eQTL data and NPDR for GWAS and quantitative outcomes}
We perform an eQTL analysis with NPDR feature selection to identify potentially interacting variants that regulate transcripts associated with MDD and demonstrate NPDR's utility for GWAS and continuous outcomes.
The MDD RNA-Seq study described above includes 915 GWAS subjects genotyped with the Illumina Omni1-Quad microarray\cite{mostafavi14}.
We use NDPR to test for the cis- (1Mb from the gene's transcription start site) and trans-eQTL influence on one of the gene expression levels associated with MDD (SCAI gene).
We included 281,648 variants following GWAS filtering.
We remove variants with a deviation from Hardyâ€“Weinberg equilibrium (P $<0.0001$ in controls) and a minor allele frequency (MAF) $<0.01$, and we use linkage disequilibrium (LD) pruning to reduce the potential bias of correlation on interaction and distance calculations.
SNPs are recursively removed within a sliding window along a given chromosome based on a pairwise LD of 0.5.
We control for MDD status in the NPDR models to isolate more direct influence of variants on expression rather than MDD association. We use the Eq.(\ref{eq:lin_reg_cov}) NPDR model and an allele mismatch operator for the SNP attribute projections\cite{titv}. 


\subsection{Software availability}
Detailed simulation and analysis code needed to reproduce the results in this study is available at {{https://github.com/lelaboratoire/npdr-paper}} (R version 3.5.0).
The \emph{npdr} R package is available at {{https://insilico.github.io/npdr/}}.

% \end{methods}

\section{Results}
\subsection{Simulation results} 

\begin{figure}[!tbp]
\centerline{\includegraphics[trim = 0 0 0 0]{../figs/fig2_pr_plots.pdf}}
\caption{{\bf Simulation comparison for detection of functional variables.} For one replicate simulation (A, top row), precision-recall curves (PRC) for continuous outcome data with main effects (left) and dichotomous outcome data with interaction effects (right). The area under the PRC (auPRC) value is reported next to each method: NPDR (magenta), Relief-based (dark blue) and random forest (light blue). The magenta $\ast$ indicates the NPDR 0.05 adjusted cutoffs (scores shown in Fig.~\ref{fig:npdr_relief}). For 100 replicate simulations (B, bottom row), the distributions of the auPRC values are compared for the methods. NPDR yields statistically significant higher auPRC than Relief or random forest ($\ast$$\ast$$\ast$ indicate P $<.0001$). All simulations use $m = 200$ samples and $p = 1,000$ attributes with 100 functional.}
\label{fig:pr_curve}
\end{figure}

\begin{figure}[!tbp]
\centerline{\includegraphics[trim = 0 0 0 0, width=0.8\textwidth]{../figs/fig1.pdf}}
\caption{{\bf Simulation comparison of importance scores.} Scatter plots of NPDR versus Relief-based scores (left) and NPDR versus random forest scores (right) for representative simulations of continuous outcome data with main effects (A, top row) and dichotomous outcome data with interaction effects (B, bottom row). Simulations use $m = 200$ samples and $p = 1,000$ attributes with 100 functional (orange). For continuous outcome (A), importance scores computed by RRelief weight, random forest percent increase in MSE and NPDR standardized linear regression coefficient ($\beta'$ from Eq.~\ref{eq:lin_reg}). For dichotomous outcome (B), scores computed by Relief-F, random forest mean decrease in accuracy and NPDR standardized logistic regression coefficient ($\beta'$ from Eq.~\ref{eq:logit_nocovar}). A regression line between the scores with correlation $r$ is displayed, and a 0.05 Bonferroni-adjusted cutoff (dashed) is shown for NPDR scores. There is no statistical threshold for Relief-based methods or random forest (area under the precision-recall curve (auPRC) is used to compare algorithm performance, see Fig.~\ref{fig:pr_curve}).}
\label{fig:npdr_relief}
\end{figure}

%\subsection{Simulation comparison for continuous outcome data} 
In simulated data with main effects and interactions for continuous and dichotomous outcomes, NPDR attribute estimates show improved precision and recall over standard Relief and random forest importance scores for detecting functional variables (Fig. \ref{fig:pr_curve}).
For one simulation, we illustrate the Precision Recall Curve (PRC) for a grid of attribute importance thresholds (Fig. \ref{fig:pr_curve}A), which shows improved area under the PRC (auPRC) for NPDR for both continuous (left) and dichotomous (right) outcomes.
Across 100 replicate simulations for each simulation type, NPDR shows significantly higher auPRC than random forest and Relief (both $P < 0.0001$, Fig. \ref{fig:pr_curve}B).
The auPRC values for all methods are higher in the interaction effect simulations relative to the main effect simulations because of a larger simulated effect size.

We use auPRC to compare other machine learning methods with NPDR because Relief and random forest lack a null distribution, whereas NPDR has an approximate distribution for hypothesis testing.
NPDR correctly detects 57 out of 100 functional attributes in a continuous-outcome main effect simulation (Fig. \ref{fig:npdr_relief}A) and 86 out of 100 functional attributes in a dichotomous-outcome interaction simulation (Fig. \ref{fig:npdr_relief}B) using an adjusted P value threshold.
Given any vertical score cutoff (Fig. \ref{fig:npdr_relief}), it is difficult for Relief or random forest to detect most of the functional attributes without including many more false positives than NPDR.
As shown by the auPRC, NPDR tends to include fewer false positives than the other methods as it detects more functional attributes (Fig. \ref{fig:pr_curve}).

NPDR importance scores are highly correlated with Relief-F scores with $r = 0.869$ for continuous outcome main effects and $0.848$ for dichotomous outcome interaction effects (Fig. \ref{fig:npdr_relief}).
This correlation is expected because both methods are nearest-neighbor based.
The correlation of NPDR with random forest scores is lower than with Relief-F, where $r = 0.692$ for continuous outcome main effects and $0.578$ for dichotomous outcome interaction effects.
The correlation for interaction effect simulations decreases further because random forest underestimates the importance of interacting attributes as the attribute dimensionality becomes large compared to the number of functional attributes~\cite{mckinney2009capturing,winham2012snp}.

% With a Bonferroni cutoff, NPDR correctly detects 57 out of 100 functional attributes in the continuous outcome (Fig. \ref{fig:npdr_relief}A) and 86 out of 100 functional attributes in the dichotomous outcome data (Fig. \ref{fig:npdr_relief}B).
% As expected, NPDR importance scores are more correlated with Relief-F ($r = 0.869, 0.848$) than with random forest ($r = 0.692, 0.578$).
% 
% Due to the lack of statistical thresholds of random forest and Relief-based methods, in practice, the importance score cutoffs for these methods are chosen arbitrarily.
% However, when a vertical cutoff is drawn for Relief-based or random forest importance score, it is difficult for random forest and Relief to detect most of the functional attributes without including many false positives, especially for random forest with interaction effects (Fig. \ref{fig:npdr_relief}).
% To explain this underwhelming performance, we recall that random forest underestimates the importance of interacting attributes as the attribute dimensionality becomes too large~\cite{mckinney2009capturing,winham2012snp}.
% 
% Meanwhile, there is a clearer separation between the functional and non-functional attributes on the NPDR importance metric.
% This improved feature selection performance of NPDR is better illustrated with the precision-recall curves (Fig. \ref{fig:pr_curve}A).
% For both datasets with different outcome and effect types, NPDR yields the highest auPRC compared to random forest and Relief-based methods.
% The auPRC of these methods in the interaction effect simulations are more than main effect simulations, which is due to differences in the simulated effect size.
% Across 100 replicated simulations for each data type, NPDR shows significantly higher auPRC than random forest and Relief (both P < 0.0001, Fig. \ref{fig:pr_curve}B).
% %Because of the structural difference between simulation of main and interaction effect, it is not trivial to [...].
% %Therefore, we aim to simulate similar correlation between NPDR and Relief-F importance score as in the case of binary outcome data with interaction effect (Fig. \ref{fig:npdr_relief}C).
% % Because Relief-based methods do not have a statistical threshold, we use the auPRC for comparison, which shows the improved feature selection performance of using a distance regression-based attribute score in NPDR compared to the standard RRelief and random forest importance score (Fig. \ref{fig:pr_curve}).
% %tend to there is a greater separation between the functional and nonfunctional group on the NPDR coefficient compared to Relief-F score.
% %In other words, if a vertical line was drawn as an arbitrary Relief-F score cutoff that yields a reasonable number of correct functional attributes, it will include at least several incorrect ones. 
% % The precision-recall curve clearly demonstrates NPDR outperforms Relief-F in this particular simulated dataset (Fig. \ref{fig:pr_curve}), and across 100 simulations of moderate effect size, NPDR yields significantly higher auPRC (P < 0.0001, see Supplementary Fig. S2).
% % While the importance scores from NPDR and Relief-F are correlated ($r = 0.869$), [any RRelief threshold (vertical line in A) will yield an equal or greater false positive rate than NDPR. ]
% Expectedly, when the effect size increases, the auPRC values of all methods approach 100\% and become closer together.
% Moreover, even though area under the Receiver Operating Characteristics curve (auROC) is not our primary metric, NPDR also gives significantly higher auROC than random forest and Relief (both P < 0.0001, see Supplementary Fig. S1).
% In summary, as the effect size varies, NPDR produces distance regression-based importance scores at least as good as if not better than random forest and Relief-based method.
% % For dichotomous outcome data with network interaction effects, we use the logistic model (Eq.~\ref{eq:logit_nocovar}) in NPDR and compare the regression coefficients with Relief-F weights in CORElearn and random forest permutation importance score (Fig. \ref{fig:npdr_relief}B).
% % We again use the fixed-$k$ neighborhood for both methods to approximate a multiSURF neighborhood.

Although our primary performance metric is auPRC, we also use the area under the Receiver Operating Characteristics curve (auROC), which also shows NPDR has statistically significant higher feature selection performance than random forest and Relief (both $P < 0.0001$, see Supplementary Fig. S2).
We also compare NPDR (using a logistic model) with STIR (based on a t-test) for the dichotomous outcome data with interaction effects (Supplementary Fig. S1).
%Across 100 simulations with specified effect size, NPDR yields significantly higher auPRC (P < 0.0001, see Supplementary Fig. S3). 
While logistic regression and the t-test have slightly different assumptions on the distribution of samples, NPDR and STIR yield highly correlated scores for dichotomous data with interaction effects, where the correlation value $r$ between the P values produced from the two methods ranges from 0.9827 to 0.9994 in 100 replications.

In the continuous and dichotomous simulations, respectively, we use NPDR with a linear model (Eq.~\ref{eq:lin_reg}) and logistic model (Eq.~\ref{eq:logit_nocovar}) to compare with standard RRelief and Relief-F (R package CORElearn) and with random forest regression and classification (permutation importance, R package randomforest).
Because CORElearn does not include the adaptive multiSURF neighborhood, we use a fixed-$k$ neighborhood $\mathcal{N}_{\bar{k}_{1/2}}$ for the Relief-based methods.
The value $\bar{k}_{1/2}=30$ (Eq.~\ref{eq:kbar}) is the expected number of nearest neighbors corresponding to a multiSURF neighborhood~\cite{bod}.
For a dataset of the size simulated in this study ($m = 200$ samples and $p = 1,000$ attributes with $100$ functional), on a desktop with an Intel Xeon W-2104 CPU and 32GB of RAM, NPDR has a 24-second and 3-second runtime for dichotomous and continuous outcome data, respectively.

\subsection{RNA-Seq NPDR analysis for MDD with confounding}
NPDR with covariate adjustment effectively removes sex-related confounding genes in the RNA-Seq study of MDD in Ref.~\cite{mostafavi14}.
We apply NPDR with the multiSURF neighborhood $\mathcal{N}_{\alpha=1/2}$ and an adjustment for the sex covariate (Eq.~\ref{eq:too_logit}).
This study contains numerous genes that are potentially confounded by sex differences.
The sex variable is significantly associated with MDD, and 485 out of the 15,231 genes are associated with sex (Bonferroni-adjusted P value $<0.0001$).
The NPDR adjustment removes the genes that are most likely spurious MDD associations due to confounding (dark points below the horizontal 0.05 adjusted significance line in Fig.~\ref{fig:npdrs_mdd}) compared to NPDR without adjustment.
Not only do these removed genes have strong differential expression based on sex, but many of these genes, such as PRKY, UTY, and USP9Y, are Y-linked and mainly expressed in testis.
For example, the RPS4Y2 ribosomal protein S4 Y-linked 2 has been shown by tissue specific studies to mainly express in prostate and testis~\cite{lopes2010human} while RPS4X (also associated with sex in the data) is most expressed in the ovary.
The NPDR runtime for this RNA-Seq dataset ($m=915$ samples and $p=15,231$ attributes) was approximately 2.3 hours on a desktop with an Intel Xeon W-2104 CPU and 32GB of RAM.
%As demonstrated in the subsequent section, when typical filtering of the attributes is applied, the runtime is greatly reduced.

%Meanwhile, highly expressed in the brain as well as testis, the gene C2orf55 (KIAA1211L) is associated with sex but remains in the adjusted NPDR list ($p_\textrm{adj} < 0.05$) and may be relevant to MDD pathophysiology.
%[XIST might be good because it is more expressed in females than males. https://www.ncbi.nlm.nih.gov/gene/7503 (see bottom of link for organ specificity).
%And RPS4X https://www.ncbi.nlm.nih.gov/gene/6191.]
%Without adjusting for sex, univariate analysis found four genes that are associated with the diagnostic phenotype: TRAT1, MCOLN3, BLCAP and ATG13, and only BLCAP is not highly associated with sex.
%A list of 485 genes with expression most associated with sex (Bonferroni-adjusted P value $<0.0001$) and their statistical significance are included in the Supplementary Table S1.

\begin{figure}[!tpb]%figure2
\centerline{\includegraphics[]{../figs/mostafavi_npdrs_mdd.pdf}}
\caption{{\bf Comparison of NPDR major depressive disorder associations with and without covariate adjustment.} Gene scatter plot of $-\log_{10}$ significance using NPDR without correction for sex (horizontal axis) and with correction for sex (vertical axis). Genes with adjusted $p_\textrm{mdd} < 0.001$ by either method are labeled. NPDR without sex correction finds 87 genes associated with MDD at the Bonferroni-adjusted 0.05 level (right of vertical dashed line), 53 of which are also significantly correlated with sex (adjusted $p_\textrm{sex} < 0.05$).  NPDR with adjustment for sex finds 56 genes associated with MDD at the Bonferroni-adjusted 0.05 level (above horizontal dashed line), 19 of which are significantly correlated with sex. The most highly associated genes with sex are eliminated by adjustment (dark genes below the horizontal dashed line) but remain in the non-adjusted set (right of dashed vertical line). }
\label{fig:npdrs_mdd}
\end{figure}

% Sensitivity to confounders extends beyond nearest-neighbors feature selection algorithms.  
% When covariates are not accounted for, even powerful methods are expected to produce biased attribute importance scores.
% We next compare sex-adjusted NPDR with random forest feature selection. 
% However, it is not trivial to include covariates in many widely used machine learning models including random forest.
% Moreover, like previous feature selection methods before STIR, importance scores produced by random forest does not follow a distribution.
% Hence, instead of P values, we compare NPDR standardized $\beta$ coefficient and random forest permutation importance score (Supplementary Fig. S4).
% Several genes with high random forest importance score ($\geq 1$) but low NPDR coefficient are highly associated with sex (dark pink - purple).
% 
% NPDR effectively handles covariates and is less susceptible to the curse of dimensionality for detecting interactions. 

% \subsection{Real-world RNA-Seq data analysis with continuous outcome (sleep quality)}
% 
% \begin{table}[]
% % \begin{tabular}{lllll}
% {\small
% \begin{tabular}{p{1.3cm}p{7.2cm}C{0.8cm}C{1.2cm}C{1.5cm}}
% Gene     & Description                                                & Beta & P value & P adjusted \\
% \hline
% ARHGEF4  & Rho guanine nucleotide exchange factor 4                   & 6.18          & 3.39e-10      & 2.01e-06       \\
% C11orf21 & Chromosome 11 open reading frame 21                        & 5.31          & 5.58e-08      & 3.30e-04       \\
% STX4     & Syntaxin 4                                                 & 5.28          & 6.55e-08      & 3.87e-04       \\
% PTPRE    & Protein tyrosine phosphatase, receptor type E              & 5.19          & 1.06e-07      & 6.28e-04       \\
% UHRF1    & Ubiquitin like with PHD and ring finger domains 1          & 5.15          & 1.32e-07      & 7.78e-04       \\
% AGAP2    & Arfgap with gtpase domain, ankyrin repeat and PH domain 2  & 4.90          & 4.78e-07      & 0.003          \\
% C16orf45 & Chromosome 16 open reading frame 45                        & 4.90          & 4.95e-07      & 0.003          \\
% DPH3P1   & Diphthamide biosynthesis 3 pseudogene 1                    & 4.75          & 1.06e-06      & 0.006          \\
% ACP5     & Acid phosphatase 5, tartrate resistant                     & 4.65          & 1.68e-06      & 0.010          \\
% CBLL1    & Cbl proto-oncogene like 1                                  & 4.65          & 1.72e-06      & 0.010          \\
% QRICH1   & Glutamine rich 1                                           & 4.56          & 2.59e-06      & 0.015          \\
% RAB7A    & Rab7a, member ras oncogene family                          & 4.53          & 2.93e-06      & 0.017          \\
% PSMD5    & Proteasome 26s subunit, non-atpase 5                       & 4.48          & 3.86e-06      & 0.023          \\
% CBY1     & Chibby family member 1, beta catenin antagonist            & 4.47          & 4.02e-06      & 0.024          \\
% CACNA1C  & Calcium voltage-gated channel subunit alpha1 C             & 4.46          & 4.08e-06      & 0.024          \\
% TUBA4B   & Tubulin alpha 4B                                           & 4.41          & 5.16e-06      & 0.031          \\
% WNT8B    & Wnt family member 8B                                       & 4.40          & 5.59e-06      & 0.033          \\
% PMEL     & Premelanosome protein                                      & 4.36          & 6.60e-06      & 0.039          \\
% EFR3B    & Efr3 homolog B                                             & 4.34          & 7.36e-06      & 0.044          \\
% MYLPF    & Myosin light chain, phosphorylatable, fast skeletal muscle & 4.33          & 7.45e-06      & 0.044          \\
% NSG1     & Neuronal vesicle trafficking associated 1                  & 4.33          & 7.66e-06      & 0.045          \\
% CHKA     & Choline kinase alpha                                       & 4.33          & 7.67e-06      & 0.045          \\
% \hline
% \end{tabular}
% }
% \caption{Genes associated with Pittsburgh Sleep Quality Index (PSQI) in NPDR analysis.}
% \label{tab:psqi}
% \end{table}
% 
% The Pittsburgh Sleep Quality Index (PSQI) measures the quality and patterns of sleep over a one-month interval, where low PSQI indicates high sleep quality.
% Patients with MDD on average have significantly higher PSQI (9.7) compared to healthy participants (5.0) (P < 0.0001, see Supplementary Fig. S3).
% NPDR identifies 24 genes significantly associated with PSQI (Table \ref{tab:psqi}).
% Among these genes, CACNA1C has been found to be associated with narcolepsy~\cite{shimada2010}, and its genetic variation has been linked to increase risk for sleep latency in infants~\cite{kantojarvi2017} and psychiatric disorders, including bipolar disorder~\cite{franke2010}, schizophrenia, recurrent major depression~\cite{green2009} and across psychiatric disorders~\cite{crossdisorder2013, lotan2014}.
% Further characterization of the interactions among these genes will prove useful in establishing a sleep-associated expression-epistasis network~\cite{lareau15, mckinney2009capturing}.
% 
% Notably, when depression status instead of sleep quality is considered, NPDR with and without sex adjustment produced results that are consistent with previously published results using the STIR algorithm (Supplementary Fig. S4)~\cite{stir}.
% We expected this consistency because the balance in sex between diagnostic groups was designed to reduce confounding.

\subsection{eQTL analysis using NPDR with GWAS and quantitative outcomes}

We perform an eQTL analysis with NPDR to demonstrate its ability to analyze continuous outcomes and SNP predictors from GWAS.
We choose to test for eQTLs that influence the expression of SCAI (cell migration and regulation of cell cycle on chromosome 9), which was one of the stronger NPDR associations with MDD (Fig. \ref{fig:npdrs_mdd}) and was found previously to have a modest cis-eQTL effect (FDR$<0.1$) in this dataset\cite{mostafavi14}.
One of the eQTLs found by NPDR (rs10997355) is an intron variant in CTNNA3 (catenin alpha-3 mediates cellâ€“cell adhesion) on chromosome 10.
In a study of schizophrenia, an intron variant near rs10997355 showed an interaction with maternal cytomegalovirus (CMV) status~\cite{borglum2014genome}.
Other studies of CTNNA3 and its nested gene LRRTM3 (encoding the Leucine-rich repeat transmembrane neuronal protein 3) have found associations with Alzheimer's disease~\cite{miyashita2007genetic} and autism spectrum disorder~\cite{wang2009common}.
The top 100 NDPR eQTLs for SCAI are provided as supplementary information.
The NPDR runtime was 7.06 hours (on a high performance computing environment with an Intel Xeon E5-2650v3 CPU, 20 nodes and 32GB of RAM).

\section{Discussion}
NPDR is the first method to our knowledge to combine projected distances and nearest-neighbors into a generalized linear model regression framework to perform feature selection.
The use of nearest neighbors enables its ability to detect interacting attributes, which is shared with Relief-based methods, but NPDR is a departure from Relief in at least five ways.
(1) For feature selection with a continuous outcome, it does not rely on the idea of a hit/miss group like current RRelief approaches~\cite{urbanowicz17}.
Rather, NPDR simply performs a regression between the outcome and attribute projected distances.
(2) For feature selection with dichotomous outcomes, NPDR uses a logistic model to fit pairwise projected distance regressors of hit and miss group.
(3) This distance-based regression formalism provides a simple mechanism for NPDR to correct for covariates, which is often neglected in machine learning and has been a limitation of Relief-based methods.
(4) For any outcome data type (dichotomous or continuous) and predictor data type, NPDR computes the statistical significance of attribute importance scores, which enables statistically based thresholds that can adjust for multiple hypothesis testing.
(5) The NPDR attribute estimator (regression coefficient) includes more variation from the projected differences than other Relief-based methods and thereby improves attribute estimate quality.
Moreover, we introduced a regularized NPDR that adds another layer of multivariate modeling to an already multi-dimensional nearest-neighbor method to shrink correlated projected attribute differences.

The novel NPDR importance score of an attribute is the standardized regression coefficient between the projected attribute differences and the numeric outcome differences between neighbors.
For continuous outcomes, the regression Relief (RRelief) importance score~\cite{robnik03} is a weighted correlation between attribute and outcome differences. This weighted correlation has a similar form to the NPDR standardized regression coefficient, which is a covariance between attribute and outcome differences divided by the variance in the outcome.
However, we provided evidence that the NPDR standardized regression coefficient is a better attribute estimator than Relief and random forest, and we showed that the NPDR models can adjust for confounding covariates and other sources of variation.

We assessed NPDR's power and ability to control false positives using realistic simulations with main effects and network interactions.
We showed that the statistical performance using NPDR P values is the same as STIR, which is limited to dichotomous outcome data.
In other words, by modeling hit/miss differences between neighbors with a logit link, NPDR can be used safely instead of STIR with the added benefit of covariate correction and the analysis of quantitative traits.
In a different Relief-based approach for continuous outcomes, Ref.~\cite{urbanowicz17} uses a standard deviation of the continuous outcome diffs to discretize the numeric outcome and make the RRelief algorithm compatible with the idea of hits and misses.
However, discretization puts constraints on the variation in numeric data that increase the risk of losing power.
NPDR uses the full variation in the continuous outcome variable, and the regression coefficient provides an interpretation in terms of variation explained while again providing flexibility for modeling additional sources of variation.

%[Include? Discuss dependency in NPDR regression? Further improvements in the NPDR covariate adjustment may be achieved by correcting the distance matrix calculation. Adding a covariate term may not be sufficient in some cases because the neighborhood could be strongly influenced by confounding genes due to the curse of dimensionality. The curse of dimensionality can also affect confounding in distance matrix calculation.]
%The study focuses on obtaining a reasonable estimate of the probability of statistically significant association between an attribute and the outcome while taking into account the complex underlying architecture of interaction among attributes. 

A related distance-based regression method is Multivariate Distance Matrix Regression (MDMR)~\cite{schork12}.
The MDMR approach uses an F-statistic to test for the association of distance matrices between two sets of factors.
The MDMR regression is performed for the distance matrix for all pairs of instances, not a subset of nearest neighbors like NPDR, which makes MDMR susceptible to missing interactions.
Its use of local neighborhoods allows NPDR to remove imposter/irrelevant instances from the neighborhood and detect interactions in the higher dimensional space. 
The ability to remove imposters from the set of nearest neighbors illustrates the ``blessings of dimensionality'' for Relief-based methods~\cite{bod}, but this class of nearest-neighbor methods is still, of course, susceptible to the curses of dimensionality~\cite{CoD}.
Another distinction between methods is that NPDR projects distances onto each attribute, allowing for hypothesis testing of individual attributes (i.e., perform feature selection), whereas MDMR focuses on specified sets of attributes.
NPDR uses the context of all attributes to compute nearest neighbors, but it focuses on the projected regression of each attribute at a time and uses the nearest neighbors to allow for detection of interactions. However, NPDR can also be used to compute the importance of sets of factors, like MDMR.
An example of this is the penalized version of NPDR that uses the set of all attributes in a nearest-neighbor projected-distance multiple regression.

NPDR can use fixed-$k$ Relief neighborhoods and radius-based Relief neighborhoods.
For fixed-$k$ neighborhoods, we expect the NPDR approach will handle imbalanced data in a less biased way than the original fixed-$k$ methods, which focus on hit/miss neighborhoods separately.
By identifying the nearest neighbors independently of hit/miss status, the neighborhood should naturally reflect the imbalance in the data.
The hit/miss status of each pair is computed separately as a categorical outcome regression variable.
This should make NPDR scores with fixed-$k$ ($\mathcal{N}_{k_\alpha}$) similar to fixed radius ($\mathcal{N}_{R_\alpha}$) for balanced and imbalanced data.
Power for detecting main effects is highest with the myopic maximum $\mathcal{N}_{k_\text{max}}$ ($k_{\text{max}}=\lfloor (m-1)/2\rfloor$).
Real biological data will likely contain a mixture of main effects and epistasis network effects~\cite{mckinney_pajewski}.
NPDR feature selection can be embedded in the backwards elimination of private Evaporative Cooling (privateEC) or in a nested cross-validation for feature selection and classification~\cite{le17} or for optimization of $\alpha$ or $k$ to balance main effect and interaction detection.

A challenge in NPDR analysis is the inherent dependence between neighbors in the projection models, which violates distribution assumptions of regression and leads to artificially lower P-values. Our results indicate that the FDR procedure effectively controls type I error despite deflated P-values. However, future studies are needed to investigate strategies to account for dependence between neighbor-pair observations such as regularization/shrinkage of the regression-coefficients\cite{de2010benchmark}. A related property of NPDR is that it is often the case that two instances will be in each other's neighborhood. In these cases, we provide an option to restrict $\mathcal{N}$ to contain only unique pairs. However, the extra sampling of neighbor projections may provide beneficial information by reinforcing tightly connected neighborhoods.  

The ability to incorporate covariates into NPDR models addresses the important but often neglected issue of confounding factors in machine learning.
We applied NPDR to a real RNA-Seq dataset for MDD to demonstrate the identification of biologically relevant genes and the removal of spurious associations by covariate correction.
NPDR with sex as a covariate adjustment successfully removed X and Y-linked genes and genes highly expressed in sex organs.
However, it is important to note that some genes removed due to a shared association with sex may be important for the pathophysiology of MDD or for classification accuracy.
Thus, covariate adjustment in NPDR is a useful option to inform a holistic analysis of a given dataset.

Application to GWAS data required no additional modifications of the algorithm other than specification of a different diff projection operator for categorical variables~\cite{titv}, and the covariate option allows principal components to be included to adjust for population structure.
One of the trans-eQTLs found by NPDR in CTNNA3 (rs10997355) for SCAI may suggest a gene-environment interaction due to maternal CMV infection.
In addition, it has been suggested that exposure to CMV may increase mood disorder risk through interactions with susceptibility variants~\cite{kim2007exposure}.
However, antibody titers for CMV were not available in this study.
To expand variant discovery and demonstrate the ability of NDPR to analyze large GWAS data, we performed a conservative LD pruning that removes some correlation between variants while still leaving a substantial number of informative variants.
Because of the omni-genic nature of NPDR, further investigation is needed to understand the effect of LD on the relative ranking of variants and the effect of correlation on nearest-neighbor calculations. 

\section*{Acknowledgements}

\section*{Funding}
This work was supported in part by the National Institute of Health Grant Nos. GM121312 and GM103456 (to BAM).

\bibliographystyle{unsrt}
\bibliography{NPDR_refs}   % name of bib file

\end{document}
